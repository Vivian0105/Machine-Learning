# -*- coding: utf-8 -*-
"""CNN_VGG16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RZ5DoiMg9HRhZzqbSX4D57k0gd-JpcmC
"""

import cv2
import os
from matplotlib import pyplot as plt
import numpy as np
from scipy.io import loadmat
import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Dataset,DataLoader,TensorDataset
import torch.nn as nn
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import torch.nn.functional as F
from torch.optim import lr_scheduler
import matplotlib.pyplot as plt

def plot_learning_curves(train_losses, valid_losses, train_accuracies, valid_accuracies,cnnName):
    fig,ax=plt.subplots()
    ax.plot(train_losses,"r-",label="Training Loss")
    ax.plot(valid_losses,"b-",label="Validation Loss")
    plt.ylabel("Loss")
    plt.xlabel("epoch")
    plt.xlim([0,len(train_losses)-1])
    plt.title("Loss Curve of %s"%(cnnName))
    legend=ax.legend()
    plt.savefig("losses_%s.png"%(cnnName))

    fig2,ax2=plt.subplots()
    ax2.plot(train_accuracies,"r-",label="Training Accuracies")
    ax2.plot(valid_accuracies,"b-",label="Validation Accuracies")
    plt.ylim(60,100)
    plt.ylabel("Accuracy")
    plt.xlabel("epoch")   
    plt.axis([0,len(train_accuracies)-1,60,100])
    plt.title("Accuracy Curve of %s"%(cnnName))
    legend=ax2.legend()
    plt.savefig('Accuracy_%s.png'%(cnnName))

class AverageMeter(object):
    def __init__(self):
        self.reset()
    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0
    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def compute_batch_accuracy(output, target):
    with torch.no_grad():
        batch_size = target.size(0)
        _, pred = torch.max(output,1)
        correct = (pred==target).sum().item()
        return correct * 100.0 / batch_size

def train(model, device, data_loader, criterion, optimizer, epoch, print_freq=300):
    losses = AverageMeter()
    accuracy = AverageMeter()
    model.train()
    for i, (input, target) in enumerate(data_loader):
        input = input.to(device)
        target = target.to(device)
        target=torch.squeeze(target)
        optimizer.zero_grad()
        output = model(input)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        losses.update(loss.item(), target.size(0))
        batch_accuracy=compute_batch_accuracy(output, target)
        accuracy.update(batch_accuracy, target.size(0))
        if i % print_freq == 0:
                print('Epoch: [{0}][{1}/{2}]\t'
                  'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                  'Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(
                        epoch, i, len(data_loader), loss=losses, acc=accuracy))
    return losses.avg, accuracy.avg

def evaluate(model, device, data_loader, criterion, print_freq=300):
    losses = AverageMeter()
    accuracy = AverageMeter()
    results = []
    model.eval()
    with torch.no_grad():
        for i, (input, target) in enumerate(data_loader):
            input = input.to(device)
            target = target.to(device)
            target=torch.squeeze(target)
            output = model(input)
            loss = criterion(output, target)
            losses.update(loss.item(), target.size(0))
            accuracy.update(compute_batch_accuracy(output, target), target.size(0))
            if i % print_freq == 0:
                print('Test: [{0}/{1}]\t''Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                  'Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(
                    i, len(data_loader),loss=losses, acc=accuracy))
    return losses.avg, accuracy.avg

def train_test_set():
    train = loadmat('train_32x32')
    test=loadmat("test_32x32")
    X=train['X'].astype(np.float32)
    Y=train['y']
    X=np.moveaxis(X,-1,0)
    X_TEST=test['X'].astype(np.float32)    
    X_TEST=np.moveaxis(X_TEST,-1,0)
    Y_TEST=test['y']
    X_11=np.load("non_digit.npy")
    print("X_11.SHAPE: ", X_11.shape)
    np.random.shuffle(X_11)
    n_train=int(0.75*X_11.shape[0])
    X_11_TRAIN=X_11[:n_train].astype(np.float32)
    X_11_TEST=X_11[n_train:].astype(np.float32)
    Y_11_TRAIN=np.array([[0]*X_11_TRAIN.shape[0]])
    Y_11_TRAIN=np.moveaxis(Y_11_TRAIN, -1,0)
    Y_11_TEST=np.array([[0]*X_11_TEST.shape[0]])
    Y_11_TEST=np.moveaxis(Y_11_TEST, -1,0)
    X=np.concatenate((X, X_11_TRAIN), axis=0)
    X=np.moveaxis(X,-1,1)
    Y=np.concatenate((Y, Y_11_TRAIN), axis=0)
    Y=Y.astype(np.int64)
    size=X.shape[0]
    seq=np.array(range(size))
    np.random.shuffle(seq)
    X_valid=X[seq[:int(0.2*size)]]
    Y_valid=Y[seq[:int(0.2*size)]]
    X_train=X[seq[int(0.2*size):]]
    Y_train=Y[seq[int(0.2*size):]]
    X_TEST=np.concatenate((X_TEST, X_11_TEST), axis=0)
    X_TEST=np.moveaxis(X_TEST,-1,1)
    Y_TEST=np.concatenate((Y_TEST, Y_11_TEST), axis=0)
    Y_TEST=Y_TEST.astype(np.int64)
    return X_train,Y_train,X_valid,Y_valid,X_TEST,Y_TEST

class DigitVGGDataset(Dataset):
    def __init__(self,X,Y):
        self.X=(X-127.5)/127.5
        self.Y=Y
        self.n_samples=self.X.shape[0]
    def __getitem__(self,index):
        sample= self.X[index]
        label = self.Y[index]
        sample= np.moveaxis(sample, 0, -1)
        sample = cv2.resize(sample,(224,224))
        sample = np.moveaxis(sample,-1,0)
        return torch.from_numpy(sample),torch.from_numpy(label)
    def __len__(self):
        return self.n_samples

num_epochs=50
batch_size=50
lr=0.005
X,Y,X_valid,Y_valid,X_TEST,Y_TEST=train_test_set()
train_dataset=DigitVGGDataset(X,Y)
valid_dataset=DigitVGGDataset(X_valid,Y_valid)
test_dataset=DigitVGGDataset(X_TEST,Y_TEST)
train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)
valid_loader=DataLoader(valid_dataset,batch_size=batch_size,shuffle=False)
test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False)

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if torch.cuda.is_available():
  print("Work on Cuda.")
model_vgg=torchvision.models.vgg16(pretrained=True)
model_vgg.classifier[-1]=nn.Linear(4096, 11)
model_vgg = model_vgg.to(device)
criterion=nn.CrossEntropyLoss()
optimizer=torch.optim.SGD(model_vgg.parameters(),lr=lr,momentum=0.9)
n_total_steps=len(train_loader)

best_val_acc = 0.0
train_losses, train_accuracies = [], []
valid_losses, valid_accuracies = [], []

previous_accuracy=0
accuracy_decrease=0
for epoch in range(num_epochs):
    train_loss, train_accuracy = train(model_vgg, device, train_loader, criterion, optimizer,epoch)
    valid_loss, valid_accuracy = evaluate(model_vgg, device, valid_loader, criterion)
    train_losses.append(train_loss)
    valid_losses.append(valid_loss)
    train_accuracies.append(train_accuracy)
    valid_accuracies.append(valid_accuracy)
    if previous_accuracy>valid_accuracy:
        accuracy_decrease+=1
    else:
        accuracy_decrease=0
    previous_accuracy=valid_accuracy
    if accuracy_decrease==1:
        break
    is_best = valid_accuracy > best_val_acc  # let's keep the model that has the best accuracy, but you can also use another metric.
    if is_best:
        best_val_acc = valid_accuracy
        torch.save(model_vgg.state_dict(), "best_vgg_model.pth")

best_vgg_model=torchvision.models.vgg16(pretrained=True)
best_vgg_model.classifier[-1]=nn.Linear(4096, 11)
best_vgg_model = best_vgg_model.to(device)
best_vgg_model.load_state_dict(torch.load('best_vgg_model.pth'))
best_vgg_model.eval()
with torch.no_grad():
    n_correct=0
    n_samples=0
    n_class_correct=[0 for i in range(11)]
    n_class_samples=[0 for i in range(11)]
    for images, labels in test_loader:
        images=images.to(device)
        labels=labels.to(device)
        labels=torch.squeeze(labels)
        if labels.size(0)<batch_size:
            break
        outputs=best_vgg_model(images)
        _,predicted=torch.max(outputs,1)
        n_samples+=labels.size(0)
        n_correct+=(predicted==labels).sum().item()
        for i in range(batch_size):
            label=labels[i]
            pred=predicted[i]
            if label == pred:
                n_class_correct[label]+=1
            n_class_samples[label]+=1
acc=n_correct/n_samples
print("Accuracy %.2f."%(acc))
for i in range(11):
    acc=n_class_correct[i]/n_class_samples[i]
    print('Accuracy of classes %i is %.2f.'%(i, acc))

plot_learning_curves(train_losses, valid_losses, train_accuracies, valid_accuracies,"VGG16")
